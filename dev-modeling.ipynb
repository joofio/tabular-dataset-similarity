{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# redo sept 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import json\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    ndcg_score,\n",
    "    cohen_kappa_score,\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "import rbo\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# import shap\n",
    "import itertools\n",
    "import scipy.stats as st\n",
    "import random\n",
    "from textdistance import (\n",
    "    levenshtein,\n",
    "    damerau_levenshtein,\n",
    "    jaro_winkler,\n",
    "    hamming,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synth_data=pd.read_csv(\"synth_pop_2.csv\",index_col=0,dtype=float).reset_index(drop=True)\n",
    "real_data = pd.read_csv(\"real_data_testing.csv\", index_col=0, dtype=float).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "###variables\n",
    "continuous_values = [\"Age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "NA_REPLACE = [\"?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_values = real_data[real_data.columns.difference(continuous_values)].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data1 = pd.read_csv(\"synth_pop_1.csv\", index_col=0, dtype=float).reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_several_feat_imp_dataset_2(\n",
    "    data,\n",
    "    categorical_cols,\n",
    "    int_cols,\n",
    "    rep=5,\n",
    "    seed=42,\n",
    "    test_size=0.05,\n",
    "    models=[DecisionTreeClassifier(), DecisionTreeRegressor()],\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    1. por cada coluna\n",
    "    2. por cada nr de repitições\n",
    "    3. treinar modelo\n",
    "    4. ir buscar feature importance\n",
    "    5. fazer a media das medias\n",
    "\n",
    "    result:{Predicted:{feature1:[v_rep1,v_rep2,v_rep3],feature2:[v_rep1,v_rep2,v_rep3]}}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    r_cols = data.columns\n",
    "    result = {}\n",
    "    #  print(result)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    for i in range(0, len(r_cols)):\n",
    "        # print(\"testing...\", r_cols[i])\n",
    "        l_feats = {k: [] for k in r_cols if k != r_cols[i]}\n",
    "        for r in range(0, rep):\n",
    "            #     print(\"rep\",r)\n",
    "            n = random.randint(0, 100)\n",
    "            # print(models[0])\n",
    "            # print(models[1])\n",
    "            if r_cols[i] in categorical_cols:\n",
    "                model = clone(models[0])\n",
    "\n",
    "                if \"random_state\" in model.get_params():\n",
    "                    model = model.set_params(random_state=np.random.randint(1, 20))\n",
    "\n",
    "            else:\n",
    "                model = clone(models[1])\n",
    "\n",
    "                if \"random_state\" in model.get_params():\n",
    "                    model = model.set_params(random_state=np.random.randint(1, 20))\n",
    "            # metric = (\n",
    "            #    \"roc_auc_score\"\n",
    "            #    if r_cols[i] in categorical_cols\n",
    "            #    else \"neg_mean_absolute_error\"\n",
    "            # )\n",
    "            X = data.drop(r_cols[i], axis=1)\n",
    "            y = data[r_cols[i]]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=n\n",
    "            )  # just for bootstrap\n",
    "            # print(X_train)\n",
    "            t = model.fit(X_train, y_train)\n",
    "\n",
    "            if hasattr(model, \"feature_importances_\"):\n",
    "                # print(r)\n",
    "                # print(t.feature_names_in_)\n",
    "                # print(t.feature_importances_)\n",
    "                # feats = {}\n",
    "                for g in zip(t.feature_names_in_, t.feature_importances_):\n",
    "                    # print(g)\n",
    "                    l_feats[g[0]].append(g[1])\n",
    "            #        print(l_feats)\n",
    "            else:\n",
    "                r = permutation_importance(\n",
    "                    t, X_train, y_train, n_repeats=15, random_state=n, n_jobs=-2\n",
    "                )\n",
    "\n",
    "                for g in zip(X_train.columns, r.importances_mean):\n",
    "                    l_feats[g[0]].append(g[1])\n",
    "\n",
    "            result[r_cols[i]] = l_feats\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scores_v2(result1, result2):\n",
    "    \"\"\"\n",
    "    does not work for more than two datasets\n",
    "    #https://towardsdatascience.com/rbo-v-s-kendall-tau-to-compare-ranked-lists-of-items-8776c5182899\n",
    "    #https://stats.stackexchange.com/questions/51295/comparison-of-ranked-lists\n",
    "    #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.weightedtau.html\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    keys = result1.keys()\n",
    "    scores_ = {}\n",
    "    for target in keys:\n",
    "        # print(col)\n",
    "        ftkeys = [key for key in keys if key != target]\n",
    "\n",
    "        #  print(result1[col])\n",
    "        m1 = {k: np.mean(v) for k, v in result1[target].items()}\n",
    "        m2 = {k: np.mean(v) for k, v in result2[target].items()}\n",
    "        # print(m1)\n",
    "        # print(m2)\n",
    "        x1_rank = st.rankdata(\n",
    "            [-1 * el for el in m1.values()], method=\"ordinal\"\n",
    "        )  # avoid tie\n",
    "        x1_rank_dict = {k: v for k, v in zip(m1.keys(), x1_rank)}\n",
    "        # print(x1_rank_dict)\n",
    "\n",
    "        x2_rank = st.rankdata(\n",
    "            [\n",
    "                -1 * el if el != 0 else el * np.random.randint(1, 10) * 0.00001 * -1\n",
    "                for el in m2.values()\n",
    "            ],\n",
    "            method=\"ordinal\",  # avoid tie\n",
    "        )  # avoid being zero\n",
    "        x2_rank_dict = {k: v for k, v in zip(m2.keys(), x2_rank)}\n",
    "\n",
    "        true_score = []\n",
    "        model_score = []\n",
    "        true_score_rank = []\n",
    "        model_score_rank = []\n",
    "        for key in ftkeys:\n",
    "            true_score_rank.append(x1_rank_dict[key])\n",
    "            model_score_rank.append(x2_rank_dict[key])\n",
    "            true_score.append(m1[key])\n",
    "            model_score.append(m2[key])\n",
    "\n",
    "        true_score_rank_join = \"\".join(str(int(e)) for e in true_score_rank)\n",
    "        model_score_rank_join = \"\".join(str(int(e)) for e in model_score_rank)\n",
    "\n",
    "        #  l_=ndcg_score([true_score_rank],[model_score])\n",
    "        n_l = ndcg_score([true_score_rank], [model_score_rank])\n",
    "\n",
    "        #\n",
    "        def mae_over_max(mae, max_):\n",
    "            if max_ == 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return mae / max_\n",
    "\n",
    "        sc = {}\n",
    "        sc[\"ndgc_score\"] = n_l\n",
    "        sc[\"cohen_kappa_score\"] = cohen_kappa_score(true_score_rank, model_score_rank)\n",
    "\n",
    "        sc[\"r2_score\"] = r2_score(true_score, model_score)\n",
    "        sc[\"levenshtein_normalized_similarity\"] = levenshtein.normalized_similarity(\n",
    "            true_score_rank, model_score_rank\n",
    "        )\n",
    "        sc[\"kendalltau\"] = st.kendalltau(true_score_rank, model_score_rank)[0]\n",
    "        sc[\"weightedtau\"] = st.weightedtau(true_score_rank, model_score_rank)[0]\n",
    "        sc[\"rbo\"] = rbo.RankingSimilarity(true_score_rank, model_score_rank).rbo()\n",
    "\n",
    "        sc[\"damerau_levenshtein_normalized_similarity\"] = (\n",
    "            damerau_levenshtein.normalized_similarity(true_score_rank, model_score_rank)\n",
    "        )\n",
    "        sc[\"jaro_winkler_normalized_similarity\"] = jaro_winkler.normalized_similarity(\n",
    "            true_score_rank, model_score_rank\n",
    "        )\n",
    "\n",
    "        sc[\"hamming_normalized_similarity\"] = hamming.normalized_similarity(\n",
    "            true_score_rank, model_score_rank\n",
    "        )\n",
    "\n",
    "        scores_[target] = {\n",
    "            \"results\": sc,\n",
    "            \"true_score\": true_score,\n",
    "            \"model_score\": model_score,\n",
    "            \"true_score_rank\": true_score_rank,\n",
    "            \"model_score_rank\": model_score_rank,\n",
    "            \"true_score_rank_join\": true_score_rank_join,\n",
    "            \"model_score_rank_join\": model_score_rank_join,\n",
    "        }\n",
    "    # for aggregated scores:\n",
    "    full_df = None\n",
    "    for k, v in scores_.items():\n",
    "        # print(scores_[k][\"results\"])\n",
    "        res_df = pd.DataFrame(scores_[k][\"results\"], index=[0])\n",
    "        if full_df is None:\n",
    "            full_df = res_df\n",
    "        else:\n",
    "            full_df = pd.concat([full_df, res_df])\n",
    "\n",
    "    full_df.loc[\"mean\"] = full_df.mean()\n",
    "\n",
    "    scores_[\"aggregated\"] = full_df.loc[\"mean\"].to_dict()\n",
    "    return scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparasion\n",
    "def get_several_dif_dataset(\n",
    "    data1_,\n",
    "    data2_,\n",
    "    categorical_cols,\n",
    "    int_cols,\n",
    "    cv,\n",
    "    models=[DecisionTreeClassifier, LinearRegression],\n",
    "):\n",
    "    \"\"\"\n",
    "    This is the gold standard as of now. It is a function that takes in two datasets and\n",
    "    returns the scores for each of the metrics.\n",
    "\n",
    "    1. preprocesses two datasets\n",
    "    2.for all columns\n",
    "    2.1 - create model on real (first)\n",
    "    2.2 - test on real and on synth\n",
    "    \"\"\"\n",
    "    data1 = data1_.copy()\n",
    "    data2 = data2_.copy()\n",
    "    le = preprocessing.OrdinalEncoder()\n",
    "    le.fit(data1_[categorical_cols].astype(str))\n",
    "    data1[categorical_cols] = le.transform(data1_[categorical_cols].astype(str))\n",
    "    # le = preprocessing.OrdinalEncoder()\n",
    "    # le.fit(data2[categorical_cols].astype(str))\n",
    "\n",
    "    data2[categorical_cols] = le.transform(data2[categorical_cols].astype(str))\n",
    "\n",
    "    r_cols = data1.columns\n",
    "    result = {}\n",
    "    for i in range(0, len(r_cols)):\n",
    "        model = (\n",
    "            models[0](random_state=42) if r_cols[i] in categorical_cols else models[1]()\n",
    "        )\n",
    "        metric = accuracy_score if r_cols[i] in categorical_cols else mean_squared_error\n",
    "        X1 = data1.drop(r_cols[i], axis=1)\n",
    "        y1 = data1[r_cols[i]]\n",
    "        X2 = data2.drop(r_cols[i], axis=1)\n",
    "        y2 = data2[r_cols[i]]\n",
    "\n",
    "        X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "            X1, y1, test_size=0.2, random_state=42\n",
    "        )\n",
    "        #  X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "        #      X2, y2, test_size=0.2, random_state=42\n",
    "        #  )\n",
    "        model.fit(X_train1, y_train1)\n",
    "        real_real = metric(y_test1, model.predict(X_test1))\n",
    "        real_synth = metric(y2, model.predict(X2))\n",
    "        # =cross_val_score(lr, X_train1, y_train1, cv=cv, scoring=metric)\n",
    "        # =cross_val_score(lr, X_train1, y2, cv=cv, scoring=metric)\n",
    "\n",
    "        result[r_cols[i]] = real_synth / real_real\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data_cross(\n",
    "    real_data, synth_data, categorical_values, continuous_values, cv\n",
    "):\n",
    "    \"\"\"\n",
    "    ???\n",
    "    \"\"\"\n",
    "    real_synth_dif = get_several_dif_dataset(\n",
    "        real_data, synth_data, categorical_values, continuous_values, cv\n",
    "    )\n",
    "    # print(real_synth_dif)\n",
    "    synth_real_dif = get_several_dif_dataset(\n",
    "        synth_data, real_data, categorical_values, continuous_values, cv\n",
    "    )\n",
    "    # print(synth_real_dif)\n",
    "    synth_real_score = {k: np.mean(v) for k, v in synth_real_dif.items()}\n",
    "    real_synth_score = {k: np.mean(v) for k, v in real_synth_dif.items()}\n",
    "    # synth_real_score_df=pd.DataFrame.from_dict(synth_real_score,orient='index',columns=[\"Metric\"])\n",
    "    # real_synth_score_df=pd.DataFrame.from_dict(real_synth_score,orient='index',columns=[\"Metric\"])\n",
    "    final_score = {\n",
    "        \"real_synth\": real_synth_dif,\n",
    "        \"synth_real\": synth_real_dif,\n",
    "        \"aggregated\": [],\n",
    "    }\n",
    "    for k, v in synth_real_score.items():\n",
    "        # print(synth_real_score[k],real_synth_score[k])\n",
    "        final_score[\"aggregated\"].append(synth_real_score[k] / real_synth_score[k])\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_two_datasets(\n",
    "    data, data_1, categorical_values, continuous_values, reps=10, seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    1. gets several feature importance for 1 dataset\n",
    "    2. Gets several feature importante for 2 dataset\n",
    "    3. calculates new scores with them\n",
    "    4. Creates Cross-validation score\n",
    "\n",
    "    \"\"\"\n",
    "    result_1 = get_several_feat_imp_dataset_2(\n",
    "        data, categorical_values, continuous_values, reps, seed=seed\n",
    "    )\n",
    "    result_2 = get_several_feat_imp_dataset_2(\n",
    "        data_1, categorical_values, continuous_values, reps, seed=seed\n",
    "    )\n",
    "    sc = create_scores_v2(result_1, result_2)\n",
    "\n",
    "    sc[\"cross\"] = aggregate_data_cross(\n",
    "        data_1, data, categorical_values, continuous_values, 10\n",
    "    )\n",
    "\n",
    "    return sc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
